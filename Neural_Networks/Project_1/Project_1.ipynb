{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the MNIST dataset using Keras\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the images to the range [0, 1]\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the shape of the third training image\n",
    "x_train[89].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you've loaded the MNIST dataset as:\n",
    "# (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the images (if not already done)\n",
    "x_train = x_train / 255.0\n",
    "\n",
    "# Select the third image and reshape it to (28, 28)\n",
    "sample = x_train[890].reshape(28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x153d7f320>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "We'll need to define 4 parameters, it is really (really) hard to know what good parameter values are on a data set for which you have no experience with, however since MNIST is pretty famous, we have some reasonable values for our data below. The parameters here are:\n",
    "\n",
    "* Learning Rate - How quickly to adjust the cost function.\n",
    "* Training Epochs - How many training cycles to go through\n",
    "* Batch Size - Size of the 'batches' of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Parameters\n",
    "\n",
    "Here we have parameters which will directly define our Neural Network, these would be adjusted depending on what your data looked like and what kind of a net you would want to build. Basically just some numbers we will eventually use to define some variables later on in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "# Assuming you've already loaded the MNIST dataset:\n",
    "# (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Get the number of training samples\n",
    "n_samples = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TensorFlow Graph Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.Input(shape=(n_input,), dtype=tf.float32, name='x')\n",
    "y = tf.keras.Input(shape=(n_classes,), dtype=tf.float32, name='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiLayer Model\n",
    "\n",
    "It is time to create our model, let's review what we want to create here.\n",
    "\n",
    "First we receive the input data array and then to send it to the first hidden layer. Then the data will begin to have a weight attached to it between layers (remember this is initially a random value) and then sent to a node to undergo an activation function (along with a Bias as mentioned in the lecture). Then it will continue on to the next hidden layer, and so on until the final output layer. In our case, we will just use two hidden layers, the more you use the longer the model will take to run (but it has more of an opportunity to possibly be more accurate on the training data).\n",
    "\n",
    "Once the transformed \"data\" has reached the output layer we need to evaluate it. Here we will use a loss function (also called a cost function) to evaluate how far off we are from the desired result. In this case, how many of the classes we got correct. \n",
    "\n",
    "Then we will apply an optimization function to minimize the cost (lower the error). This is done by adjusting weight values accordingly across the network. In out example, we will use the [Adam Optimizer](http://arxiv.org/pdf/1412.6980v8.pdf), which keep in mind, relative to other mathematical concepts, is an extremely recent development.\n",
    "\n",
    "We can adjust how quickly to apply this optimization by changing our earlier learning rate parameter. The lower the rate the higher the possibility for accurate training results, but that comes at the cost of having to wait (physical time wise) for the results. Of course, after a certain point there is no benefit to lower the learning rate.\n",
    "\n",
    "Now we will create our model, we'll start with 2 hidden layers, which use the [RELU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks) activation function, which is a very simple rectifier function which essentially either returns x or zero. For our final output layer we will use a linear activation with matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x, weights, biases):\n",
    "    '''\n",
    "    x : Input tensor\n",
    "    weights: Dictionary of weights\n",
    "    biases: Dictionary of biases\n",
    "    '''\n",
    "    \n",
    "    # First hidden layer with ReLU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    # Second hidden layer with ReLU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out_h'])+biases['out_b']\n",
    "    \n",
    "    return out_layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Bias\n",
    "\n",
    "In order for our tensorflow model to work we need to create two dictionaries containing our weight and bias objects for the model. We can use the **tf.variable** object type. This is different from a constant because TensorFlow's Graph Object becomes aware of the states of all the variables. A Variable is a modifiable tensor that lives in TensorFlow's graph of interacting operations. It can be used and even modified by the computation. We will generally have the model parameters be Variables. From the documentation string:\n",
    "\n",
    "    A variable maintains state in the graph across calls to `run()`. You add a variable to the graph by constructing an instance of the class `Variable`.\n",
    "\n",
    "    The `Variable()` constructor requires an initial value for the variable, which can be a `Tensor` of any type and shape. The initial value defines the type and shape of the variable. After construction, the type and shape of the variable are fixed. The value can be changed using one of the assign methods.\n",
    "    \n",
    "We'll use tf's built-in random_normal method to create the random values for our weights and biases (you could also just pass ones as the initial biases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use tf.random.normal instead of tf.random_normal\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random.normal([n_input, n_hidden_1]), name='h1'),\n",
    "    'h2': tf.Variable(tf.random.normal([n_hidden_1, n_hidden_2]), name='h2'),\n",
    "    'out': tf.Variable(tf.random.normal([n_hidden_2, n_classes]), name='out_h')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tf.random.normal instead of tf.random_normal\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random.normal([n_hidden_1]), name='b1'),\n",
    "    'b2': tf.Variable(tf.random.normal([n_hidden_2]), name='b2'),\n",
    "    'out': tf.Variable(tf.random.normal([n_classes]), name='out_b')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(shape=(n_input,)),  # Input layer\n",
    "    tf.keras.layers.Dense(n_hidden_1, activation='relu', name='hidden_layer_1'),  # First hidden layer\n",
    "    tf.keras.layers.Dense(n_hidden_2, activation='relu', name='hidden_layer_2'),  # Second hidden layer\n",
    "    tf.keras.layers.Dense(n_classes, activation=None, name='output_layer')  # Output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ hidden_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">200,960</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ hidden_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ hidden_layer_1 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m200,960\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ hidden_layer_2 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_layer (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m2,570\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">269,322</span> (1.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m269,322\u001b[0m (1.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">269,322</span> (1.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m269,322\u001b[0m (1.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Define your model as previously shown\n",
    "# (Assuming the model has already been defined as `model`)\n",
    "\n",
    "# Define learning rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Compile the model with loss and optimizer\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# You can view the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m4000/4000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 822us/step - accuracy: 0.9090 - loss: 0.3141\n",
      "Epoch 2/10\n",
      "\u001b[1m4000/4000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 845us/step - accuracy: 0.9712 - loss: 0.0978\n",
      "Epoch 3/10\n",
      "\u001b[1m4000/4000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 821us/step - accuracy: 0.9802 - loss: 0.0648\n",
      "Epoch 4/10\n",
      "\u001b[1m4000/4000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 816us/step - accuracy: 0.9850 - loss: 0.0512\n",
      "Epoch 5/10\n",
      "\u001b[1m4000/4000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 823us/step - accuracy: 0.9870 - loss: 0.0418\n",
      "Epoch 6/10\n",
      "\u001b[1m4000/4000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 867us/step - accuracy: 0.9900 - loss: 0.0300\n",
      "Epoch 7/10\n",
      "\u001b[1m4000/4000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 808us/step - accuracy: 0.9904 - loss: 0.0283\n",
      "Epoch 8/10\n",
      "\u001b[1m4000/4000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 833us/step - accuracy: 0.9917 - loss: 0.0266\n",
      "Epoch 9/10\n",
      "\u001b[1m4000/4000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 815us/step - accuracy: 0.9930 - loss: 0.0214\n",
      "Epoch 10/10\n",
      "\u001b[1m4000/4000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 829us/step - accuracy: 0.9933 - loss: 0.0225\n",
      "Model has completed 10 Epochs of Training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras import datasets\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = X_train.reshape(-1, 28 * 28).astype('float32') / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "\n",
    "\n",
    "# Reshape X_test if necessary for model input\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)  # For grayscale images, reshape to (num_samples, height, width, channels)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "# Define your model (as previously shown)\n",
    "# (Assuming the model has already been defined and compiled)\n",
    "\n",
    "# Define parameters\n",
    "batch_size = 15\n",
    "training_epochs = 10\n",
    "\n",
    "# Create a TensorFlow Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, epochs=training_epochs)\n",
    "\n",
    "print(\"Model has completed {} Epochs of Training\".format(training_epochs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluations\n",
    "\n",
    "Tensorflow comes with some built-in functions to help evaluate our model, including tf.equal and tf.cast with tf.reduce_mean.\n",
    "\n",
    "**tf.equal()**\n",
    "\n",
    "This is essentially just a check of predictions == y_test. In our case since we know the format of the labels is a 1 in an array of zeroes, we can compare argmax() location of that 1. Remember that **y** here is still that placeholder we created at the very beginning, we will perform a series of operations to get a Tensor that we can eventually fill in the test data for with an evaluation method. What we are currently running will still be empty of test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378us/step\n"
     ]
    }
   ],
   "source": [
    "X_test = X_test.reshape(-1, 28 * 28).astype('float32') / 255.0  # Flattened inpu\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342us/step - accuracy: 0.9699 - loss: 0.1777\n",
      "Test Loss: 0.14727112650871277\n",
      "Test Accuracy: 0.9736999869346619\n",
      "Confusion Matrix:\n",
      "tf.Tensor(\n",
      "[[ 971    1    0    1    1    2    2    0    1    1]\n",
      " [   0 1122    1    1    0    2    2    0    7    0]\n",
      " [   2    1 1015    3    1    0    0    4    5    1]\n",
      " [   0    0    3  982    0   10    0    5    2    8]\n",
      " [   2    1    3    0  947    4    8    2    1   14]\n",
      " [   2    0    0    5    2  876    1    0    4    2]\n",
      " [   2    1    0    1    2   40  908    1    2    1]\n",
      " [   1    5    5    1    2    1    0 1006    5    2]\n",
      " [   2    1    2    4    7   14    0    1  942    1]\n",
      " [   4    2    0    3   10   11    2    6    3  968]], shape=(10, 10), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = tf.argmax(y_pred, axis=1)\n",
    "y_true_classes = tf.argmax(y_test, axis=1)\n",
    "\n",
    "# Evaluate the model on the test set (for accuracy and loss)\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print test accuracy and loss\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Confusion Matrix using TensorFlow\n",
    "conf_matrix = tf.math.confusion_matrix(labels=y_true_classes, predictions=y_pred_classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Additional metrics using TensorFlow (Precision, Recall, F1-Score)\n",
    "precision = tf.keras.metrics.Precision()\n",
    "recall = tf.keras.metrics.Recall()\n",
    "precision.update_state(y_true_classes, y_pred_classes)\n",
    "recall.update_state(y_true_classes, y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_value = precision.result().numpy()\n",
    "recall_value = recall.result().numpy()\n",
    "f1_score = 2 * (precision_value * recall_value) / (precision_value + recall_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9990015625953674\n",
      "Recall: 0.9983370304107666\n",
      "F1-Score: 0.9986692243279425\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision: {precision_value}\")\n",
    "print(f\"Recall: {recall_value}\")\n",
    "print(f\"F1-Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
